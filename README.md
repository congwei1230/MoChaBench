# MoChaBench

This repo contains the `Benchmark`, `Evaluation Code` and `MoCha's Generation Results` for [MoCha
Towards Movie-Grade Talking Character Synthesis](https://arxiv.org/pdf/2503.23307).

<a target="_blank" href="https://arxiv.org/pdf/2503.23307">
<img style="height:19pt" src="https://img.shields.io/badge/-Paper-black?style=flat&logo=arxiv"></a>
<a target="_blank" href="https://congwei1230.github.io/MoCha/">
<img style="height:19pt" src="https://img.shields.io/badge/-üåê%20Website-black?style=flat"></a>
<a target="_blank" href="https://huggingface.co/datasets/CongWei1230/MoChaBench">
<a target="_blank" href="https://github.com/congwei1230/MoChaBench/tree/main/benchmark">
<img style="height:19pt" src="https://img.shields.io/badge/-MoChaBench-red?style=flat&logo=github"></a>
<a target="_blank" href="https://github.com/congwei1230/MoChaBench/tree/main/mocha-generation">
<img style="height:19pt" src="https://img.shields.io/badge/-Our Results on MoChaBench-red?style=flat&logo=github"></a>
<br>
<a target="_blank" href="https://huggingface.co/datasets/CongWei1230/MoCha-Generation-on-MoChaBench">
<img style="height:19pt" src="https://img.shields.io/badge/-ü§ó%20Visualizing Our Results on MoChaBench-yellow?style=flat"></a>
<a target="_blank" href="https://huggingface.co/datasets/CongWei1230/MoChaBench">
<img style="height:19pt" src="https://img.shields.io/badge/-ü§ó%20Visualizing MoChaBench-yellow?style=flat"></a>
<br> 
Thanks to the community for sharing ‚Äî 

[An emotional narrative](https://x.com/CongWei1230/status/1907879690959732878)
, created with light manual editing on clips generated by **MoCha**, has surpassed **1 million views** on X. <br>
<a target="_blank" href="https://x.com/AngryTomtweets/status/1907036631057752164">
<img style="height:16pt" src="https://img.shields.io/badge/-Tweet1-blue?style=flat&logo=twitter"></a>
<a target="_blank" href="https://x.com/_akhaliq/status/1906935462075236621">
<img style="height:16pt" src="https://img.shields.io/badge/-Tweet2-blue?style=flat&logo=twitter"></a>
<a target="_blank" href="https://x.com/minchoi/status/1907265748721889383">
<img style="height:16pt" src="https://img.shields.io/badge/-Tweet3-blue?style=flat&logo=twitter"></a>
<br>

<div align="center">
  <img src="assets/narrative.gif" alt="MoCha" width="70%"/>
</div>

### üèÜ MoCha Benchmark Leaderboard

| Task |  Model | Sync-Dist. ‚Üì | Sync-Conf. ‚Üë |
|--------|--------|----------------|-----------------|
| üßë Single-Character Monologue (English) <br><sub>Subtasks: 1p_camera_movement, 1p_closeup_facingcamera, <br>1p_emotion, 1p_mediumshot_actioncontrol, 1p_portrait, 2p_1clip_1talk</sub> | MoCha | **8.185** | **6.333** |
| üë• Multi-Character Turn-based Dialogue (English) <br><sub>Subtasks: 2p_2clip_2talk</sub> | MoCha | **8.601** | **4.951** |

### Per-SubTask Averages

| SubTask                      | Model | Sync-Dist. ‚Üì | Sync-Conf. ‚Üë | Example (n) |
|-----------------------------|--------|----------------|-----------------|----------------|
| 1p_camera_movement           | MoCha | 8.455          | 5.432           | 18             |
| 1p_closeup_facingcamera      | MoCha | 7.958          | 6.298           | 27             |
| 1p_emotion                   | MoCha | 8.073          | 6.214           | 34             |
| 1p_generalize_chinese        | MoCha | 8.273          | 4.398           | 4              |
| 1p_mediumshot_actioncontrol  | MoCha | 8.386          | 6.241           | 52             |
| 1p_protrait                  | MoCha | 8.125          | 6.892           | 38             |
| 2p_1clip_1talk               | MoCha | 8.082          | 6.493           | 30             |
| 2p_2clip_2talk               | MoCha | 8.601          | 4.951           | 15             |



## Evaluate Lip Sync Scores with SyncNet:

### Download this repo
[SyncNet Weights](https://github.com/congwei1230/MoChaBench/tree/main/eval-lipsync/weights), [Benchmark](https://github.com/congwei1230/MoChaBench/tree/main/benchmark) and [MoCha's Generation Results](https://github.com/congwei1230/MoChaBench/tree/main/mocha-generation) are embedded in this git repo
```
git clone https://github.com/congwei1230/MoChaBench.git
```

### Dependencies
```
conda create -n mochabench_eval python=3.8
conda activate mochabench_eval
pip install -r requirements.txt
# require ffmpeg installed
```

### Overview
The SyncNet codebase is adapted from [joonson/syncnet_python](https://github.com/joonson/syncnet_python) for improved API and code structure.

Follows a HuggingFace Diffuser-style structure.
We provided a
`SyncNetPipeline` Class located at `eval-lipsync\script\syncnet_pipeline.py`.

`SyncNetPipeline` can be intialized by providing the weights and configs.

```
pipe = SyncNetPipeline(
    {
        "s3fd_weights":  "../weights/sfd_face.pth",
        "syncnet_weights": "../weights/syncnet_v2.model",
    },
    device="cuda",          # or "cpu"
)
```
It has an `inference` function to score a single pair of video and speech(with speech denoised from the audio)
```
results = pipe.inference(
    video_path="../example/video.avi",   # RGB video
    audio_path="../example/speech.wav",   # speech track (any ffmpeg-readable format)
    cache_dir="../example/cache",    # optional; omit to auto-cleanup intermediates
)
```

### Example Script to run SyncNetPipeline on single pair of (video, speech)

```
cd script
python run_syncnet_pipeline_on_1example.py
```
You are expected to get values close (¬±0.1 due to ffmpeg version, the version i am using `ffmpeg version 7.1.1-essentials_build-www.gyan.dev Copyright (c) 2000-2025 the FFmpeg developers`) to:
```
AV offset:      1
Min dist:       9.255
Confidence:     4.497
best-confidence   : 4.4973907470703125
lowest distance   : 9.255396842956543
per-crop offsets  : [1]
```

### Running SyncNetPipeline on MoCha-Generated Videos for MoChaBench Evaluation

```
cd eval-lipsync\script
python run_syncnet_pipeline_on_mocha_generation_on_mocha_bench.py
```

### Running SyncNetPipeline on Your Model‚Äôs Outputs for MoChaBench
You need to create a folder similar to the structure of ``local_repo_dir/remocha-generation``
Then modify the 
```
cd eval-lipsync\script
python run_syncnet_pipeline_on_mocha_generation_on_mocha_bench.py
```

