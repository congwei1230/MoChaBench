# MoChaBench

This repo contains the `Benchmark`, **`Standard Evaluation Codebase`** and `MoCha's Generation Results` for [MoCha
Towards Movie-Grade Talking Character Synthesis](https://arxiv.org/pdf/2503.23307).

<a target="_blank" href="https://arxiv.org/pdf/2503.23307">
<img style="height:19pt" src="https://img.shields.io/badge/-Paper-black?style=flat&logo=arxiv"></a>
<a target="_blank" href="https://congwei1230.github.io/MoCha/">
<img style="height:19pt" src="https://img.shields.io/badge/-ğŸŒ%20Website-black?style=flat"></a>
<a target="_blank" href="https://huggingface.co/datasets/CongWei1230/MoChaBench">
<a target="_blank" href="https://github.com/congwei1230/MoChaBench/tree/main/benchmark">
<img style="height:19pt" src="https://img.shields.io/badge/-MoChaBench-red?style=flat&logo=github"></a>
<a target="_blank" href="https://github.com/congwei1230/MoChaBench/tree/main/mocha-generation">
<img style="height:19pt" src="https://img.shields.io/badge/-Our Results on MoChaBench-red?style=flat&logo=github"></a>
<br>
<a target="_blank" href="https://huggingface.co/datasets/CongWei1230/MoCha-Generation-on-MoChaBench">
<img style="height:19pt" src="https://img.shields.io/badge/-ğŸ¤—%20Visualizing Our Results on MoChaBench-yellow?style=flat"></a>
<a target="_blank" href="https://huggingface.co/datasets/CongWei1230/MoChaBench">
<img style="height:19pt" src="https://img.shields.io/badge/-ğŸ¤—%20Visualizing MoChaBench-yellow?style=flat"></a>
<br> 

Many thanks to the community for sharing â€” 
[An emotional narrative](https://x.com/CongWei1230/status/1907879690959732878), created with light manual editing on clips generated by **MoCha**, has surpassed **1 million views** on X. <br>
<a target="_blank" href="https://x.com/AngryTomtweets/status/1907036631057752164">
<img style="height:16pt" src="https://img.shields.io/badge/-Tweet1-blue?style=flat&logo=twitter"></a>
<a target="_blank" href="https://x.com/_akhaliq/status/1906935462075236621">
<img style="height:16pt" src="https://img.shields.io/badge/-Tweet2-blue?style=flat&logo=twitter"></a>
<a target="_blank" href="https://x.com/minchoi/status/1907265748721889383">
<img style="height:16pt" src="https://img.shields.io/badge/-Tweet3-blue?style=flat&logo=twitter"></a>
<br>

<div align="center">
  <img src="assets/narrative.gif" alt="MoCha" width="70%"/>
</div>

<br>

# ğŸ† MoChaBench Leaderboard


### ğŸ§‘ Single-Character Monologue (English)
Including categories: 1p_camera_movement, 1p_closeup_facingcamera, 1p_emotion, 1p_mediumshot_actioncontrol, 1p_portrait, 2p_1clip_1talk
| Method | Sync-Conf. â†‘ | Sync-Dist. â†“ |
|-------|--------------|--------------|
| MoCha | **6.333**    | **8.185**      |
| Hallo3 | 4.866    | 8.963       |
| SadTalker |4.727      |9.239         |
| AniPortrait| 1.740    | 11.383 |
---

### ğŸ‘¥ Multi-Character Turn-based Dialogue (English)
Including categories: 2p_2clip_2talk</sub>

| Method | Sync-Conf. â†‘ | Sync-Dist. â†“ |
|-------|--------------|--------------|
| MoCha | **4.951**    | **8.601**    |
---

### Per-Category Averages

| Category                      | Model | Sync-Dist. â†“ | Sync-Conf. â†‘ | Example (n) |
|-----------------------------|--------|----------------|-----------------|----------------|
| 1p_camera_movement           | MoCha | 8.455          | 5.432           | 18             |
| 1p_closeup_facingcamera      | MoCha | 7.958          | 6.298           | 27             |
| 1p_emotion                   | MoCha | 8.073          | 6.214           | 34             |
| 1p_generalize_chinese        | MoCha | 8.273          | 4.398           | 4              |
| 1p_mediumshot_actioncontrol  | MoCha | 8.386          | 6.241           | 52             |
| 1p_protrait                  | MoCha | 8.125          | 6.892           | 38             |
| 2p_1clip_1talk               | MoCha | 8.082          | 6.493           | 30             |
| 2p_2clip_2talk               | MoCha | 8.601          | 4.951           | 15             |



# â–¶ï¸ Evaluating Lip Sync Scores

## Overview
We use SyncNet for evaluation. The codebase is adapted from [joonson/syncnet_python](https://github.com/joonson/syncnet_python) with **improved code structure and a unified API** to facilitate evaluation for the community.

The implementation follows a Hugging Face Diffusers-style structure.
We provided a
`SyncNetPipeline` Class, located at `eval-lipsync\script\syncnet_pipeline.py`.

You can initialize `SyncNetPipeline` by providing the weights and configs:
```python
pipe = SyncNetPipeline(
    {
        "s3fd_weights":  "../weights/sfd_face.pth",
        "syncnet_weights": "../weights/syncnet_v2.model",
    },
    device="cuda",          # or "cpu"
)
```
The pipeline offers an `inference` function to score a single pair of video and speech. For fair comparison, the input speech should be a denoised vocal source extracted from your audio. You can use seperator like [Kim_Vocal_2](https://huggingface.co/huangjackson/Kim_Vocal_2) for general noise remvoal and [Demucs_mdx_extra](https://github.com/facebookresearch/demucs) for music removal
```python
results = pipe.inference(
    video_path="../example/video.avi",   # RGB video
    audio_path="../example/speech.wav",   # speech track (must be denoised from audio, ffmpeg-readable format)
    cache_dir="../example/cache",    # optional; omit to auto-cleanup intermediates
)
```

## Benchmark files
We provide the benchmark files in the `benchmark/` directory, organized by data type and category.

Each file follows the structure:  
`benchmark/<data_type>/<category>/<context_id>.<ext>`

<details>
<summary>Directory Structure</summary>

```cmd
â”œâ”€benchmark
â”‚  â”œâ”€audios
â”‚  â”‚  â”œâ”€1p_camera_movement
â”‚  â”‚  â”‚  â”œâ”€ 10_man_basketball_camera_push_in.wav
â”‚  â”‚  â”‚  ...
â”‚  â”‚  â”œâ”€1p_closeup_facingcamera
â”‚  â”‚  â”œâ”€1p_emotion
â”‚  â”‚  â”œâ”€1p_generalize_chinese
â”‚  â”‚  â”œâ”€1p_mediumshot_actioncontrol
â”‚  â”‚  â”œâ”€1p_protrait
â”‚  â”‚  â”œâ”€2p_1clip_1talk
â”‚  â”‚  â””â”€2p_2clip_2talk
â”‚  â”œâ”€first-frames-from-mocha-generation
â”‚  â”‚  â”œâ”€1p_camera_movement
â”‚  â”‚  â”‚  â”œâ”€ 10_man_basketball_camera_push_in.png
â”‚  â”‚  â”‚  ...
â”‚  â”‚  â”œâ”€1p_closeup_facingcamera
â”‚  â”‚  â”œâ”€1p_emotion
â”‚  â”‚  â”œâ”€1p_generalize_chinese
â”‚  â”‚  â”œâ”€1p_mediumshot_actioncontrol
â”‚  â”‚  â”œâ”€1p_protrait
â”‚  â”‚  â”œâ”€2p_1clip_1talk
â”‚  â”‚  â””â”€2p_2clip_2talk
â”‚  â””â”€speeches
â”‚      â”œâ”€1p_camera_movement
|      |  â”œâ”€ 10_man_basketball_camera_push_in_speech.wav
â”‚      â”‚  ...
â”‚      â”œâ”€1p_closeup_facingcamera
â”‚      â”œâ”€1p_emotion
â”‚      â”œâ”€1p_generalize_chinese
â”‚      â”œâ”€1p_mediumshot_actioncontrol
â”‚      â”œâ”€1p_protrait
â”‚      â”œâ”€2p_1clip_1talk
â”‚      â””â”€2p_2clip_2talk
â””â”€â”€benchmark.csv
```
</details>

- **`benchmark.csv`** contains metadata for each sample, with each row specifying:  
  `idx_in_category`, `category`, `context_id`, `prompt`.
    
    We use `benchmark.csv` to connect files. Any file in the benchmark can be located using the combination:  
  `/benchmark/<data_type>/<category>/<context_id>.<ext>`

- `speeches` files are generated from `audios` files by using [Demucs_mdx_extra](https://github.com/facebookresearch/demucs). For fair comparison, `speeches` should also be used as the input to your own model instead of `audios`.

- We also provie `first-frames-from-mocha-generation` to facilitate fair comparison for (image + text + audio â†’ video) models. 

## How to Use
### Download this repo
[SyncNet Weights](https://github.com/congwei1230/MoChaBench/tree/main/eval-lipsync/weights), [Benchmark](https://github.com/congwei1230/MoChaBench/tree/main/benchmark) and [MoCha's Generation Results](https://github.com/congwei1230/MoChaBench/tree/main/mocha-generation) are embedded in this git repo
```sh
git clone https://github.com/congwei1230/MoChaBench.git
```

### Dependencies
```sh
conda create -n mochabench_eval python=3.8
conda activate mochabench_eval
cd eval-lipsync
pip install -r requirements.txt
# require ffmpeg installed
```



### Example Script to run SyncNetPipeline on single pair of (video, speech)

```sh
cd script
python run_syncnet_pipeline_on_1example.py
```
You are expected to get values close (Â±0.1 due to ffmpeg version, the version i am using `ffmpeg version 7.1.1-essentials_build-www.gyan.dev Copyright (c) 2000-2025 the FFmpeg developers`) to:
```
AV offset:      1
Min dist:       9.255
Confidence:     4.497
best-confidence   : 4.4973907470703125
lowest distance   : 9.255396842956543
per-crop offsets  : [1]
```

### Running SyncNetPipeline on MoCha-Generated Videos for MoChaBench Evaluation

We provide the MoCha-Generated videos in the `mocha-generation/` directory

Each file follows the structure:  
`mocha-generation/<category>/<context_id>.<ext>`

```cmd
mocha-generation
    â”œâ”€1p_camera_movement
    â”‚   â”œâ”€ 10_man_basketball_camera_push_in.mp4
    â”‚   â”‚  ...
    â”œâ”€1p_closeup_facingcamera
    â”œâ”€1p_emotion
    â”œâ”€1p_generalize_chinese
    â”œâ”€1p_mediumshot_actioncontrol
    â”œâ”€1p_protrait
    â”œâ”€2p_1clip_1talk
    â””â”€2p_2clip_2talk
```

To evaluate the results, simply run the pipeline below.  
This script will print the score for each category, as well as the average scores for Monologue and Dialogue.
It will also output a CSV file at `eval-lipsync/mocha-eval-results/sync_scores.csv`, recording each exampleâ€™s score.
```sh
cd eval-lipsync/script
python run_syncnet_pipeline_on_mocha_generation_on_mocha_bench.py
```


### Running SyncNetPipeline on Your Modelâ€™s Outputs for MoChaBench

To evaluate your own modelâ€™s outputs with MoChaBench, first use the following inputs to generate videos:
- **Speech input:** `benchmark/speeches`
- **Text input:** `prompt` from `benchmark.csv`
- **Image input:** `benchmark/first-frames-from-mocha-generation` (if your model requires an image condition)

Then organize your generated videos in a folder that matches the structure of `mocha-generation/`:

```cmd
<your_outputs_dir>/
    â”œâ”€ 1p_camera_movement/
    â”‚    â”œâ”€ 10_man_basketball_camera_push_in.mp4
    â”‚    â”‚ ...
    â”œâ”€ 1p_closeup_facingcamera/
    â”œâ”€ 1p_emotion/
    â”œâ”€ 1p_generalize_chinese/
    â”œâ”€ 1p_mediumshot_actioncontrol/
    â”œâ”€ 1p_protrait/
    â”œâ”€ 2p_1clip_1talk/
    â””â”€ 2p_2clip_2talk/
```

Each video should be named as `<context_id>.mp4` within the corresponding category folder. You donâ€™t need to provide an mp4 for every categoryâ€”the script will skip any missing videos and report scores for the rest.

Next, modify the script `run_syncnet_pipeline_on_your_own_model_results.py` to point to your video folder.

Then, run:

```sh
cd eval-lipsync/script
python run_syncnet_pipeline_on_your_own_model_results.py
```
The script will output a CSV file at `eval-lipsync/your own model-eval-results/sync_scores.csv` with the evaluation scores for each example.


## ğŸ“š Citation

ğŸŒŸ If you find our work helpful, please leave us a star and cite our paper.

```bibtex
@article{wei2025mocha,
  title={MoCha: Towards Movie-Grade Talking Character Synthesis},
  author={Wei, Cong and Sun, Bo and Ma, Haoyu and Hou, Ji and Juefei-Xu, Felix and He, Zecheng and Dai, Xiaoliang and Zhang, Luxin and Li, Kunpeng and Hou, Tingbo and others},
  journal={arXiv preprint arXiv:2503.23307},
  year={2025}
}
```